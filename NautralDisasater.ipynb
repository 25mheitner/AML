{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXNT9UcVXVR6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization,SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
        "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
        "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "1KX1u2GuXaXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "IYUL2c_QXb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_val = pd.DataFrame(train.isnull().sum())\n",
        "missing_val = missing_val.reset_index()\n",
        "missing_val"
      ],
      "metadata": {
        "id": "lnOiTv61Xe-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "XH0AIdLTXgVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe().T"
      ],
      "metadata": {
        "id": "6G6B4_nQXhhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return re.sub(html, '', text)\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "# Applying helper functions\n",
        "\n",
        "train['clean_text'] = train['text'].apply(lambda x: remove_URL(x))\n",
        "train['clean_text'] = train['clean_text'].apply(lambda x: remove_emoji(x))\n",
        "train['clean_text'] = train['clean_text'].apply(lambda x: remove_html(x))\n",
        "train['clean_text'] = train['clean_text'].apply(lambda x: remove_punct(x))"
      ],
      "metadata": {
        "id": "f2WTgJ-LXjfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['tokenized'] = train['clean_text'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "66eIMU6SXlUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "XgObjMrcXmoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['lower'] = train['tokenized'].apply(\n",
        "    lambda x: [word.lower() for word in x])\n",
        "\n",
        "train['no_stopwords'] = train['lower'].apply(\n",
        "    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])"
      ],
      "metadata": {
        "id": "wdphIBoPXn5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['no_stopwords'] = [' '.join(map(str, l)) for l in train['no_stopwords']]"
      ],
      "metadata": {
        "id": "iswjWmCIXpuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "d3akfabXXq-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['clean_text'] = test['text'].apply(lambda x: remove_URL(x))\n",
        "test['clean_text'] = test['clean_text'].apply(lambda x: remove_emoji(x))\n",
        "test['clean_text'] = test['clean_text'].apply(lambda x: remove_html(x))\n",
        "test['clean_text'] = test['clean_text'].apply(lambda x: remove_punct(x))\n",
        "\n",
        "test['tokenized'] = test['clean_text'].apply(word_tokenize)\n",
        "\n",
        "test['lower'] = test['tokenized'].apply(\n",
        "    lambda x: [word.lower() for word in x])\n",
        "\n",
        "test['no_stopwords'] = test['lower'].apply(\n",
        "    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])\n",
        "\n",
        "test['no_stopwords'] = [' '.join(map(str, l)) for l in test['no_stopwords']]"
      ],
      "metadata": {
        "id": "FodER_8fXtBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "combined = train['no_stopwords'].tolist() + test['no_stopwords'].tolist()"
      ],
      "metadata": {
        "id": "yg6oUxeJXu2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(combined)"
      ],
      "metadata": {
        "id": "IVMNfg8rXwOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target distribution.\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\n",
        "sns.countplot(train['target'], ax=axes[0])\n",
        "axes[1].pie(train['target'].value_counts(),\n",
        "            labels=['Not Disaster', 'Disaster'],\n",
        "            autopct='%1.2f%%',\n",
        "            shadow=True,\n",
        "            explode=(0.05, 0),\n",
        "            startangle=60)\n",
        "fig.suptitle('Distribution of the Tweets', fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SntmWiCfXyPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "67NIghIrX0eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "plt.figure(figsize=(12,6))\n",
        "text = ' '.join(train.no_stopwords[train['target']==1])\n",
        "wc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "a--rLiqyX2Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "plt.figure(figsize=(12,6))\n",
        "text = ' '.join(train.no_stopwords[train['target']==0])\n",
        "wc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "p5rOhPKwX3N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "tweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\n",
        "ax1.hist(tweet_len,color='blue')\n",
        "ax1.set_title('disaster tweets')\n",
        "tweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\n",
        "ax2.hist(tweet_len,color='yellow')\n",
        "ax2.set_title('Non disaster tweets')\n",
        "fig.suptitle('Words in a processed tweet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uunj-R_UX4fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "word=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='purple')\n",
        "ax1.set_title('disaster tweets')\n",
        "word=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='orange')\n",
        "ax2.set_title('Non disaster tweets')\n",
        "fig.suptitle('Average word length in each processed tweet')"
      ],
      "metadata": {
        "id": "o4ekmdquX6QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = dict()\n",
        "f = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "5pVAyw42X7vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_tweet = 50\n",
        "\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(combined)\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "encoded_tweet = tok.texts_to_sequences(combined)\n",
        "padded_tweet = pad_sequences(encoded_tweet, maxlen=max_len_tweet, padding='post')\n",
        "\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "\n",
        "tweet_embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tok.word_index.items():\n",
        "    t_embedding_vector = embeddings_index.get(word)\n",
        "    if t_embedding_vector is not None:\n",
        "        tweet_embedding_matrix[i] = t_embedding_vector"
      ],
      "metadata": {
        "id": "eK1MBQGnX-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_len_tweet, embeddings_initializer=Constant(tweet_embedding_matrix), trainable=False))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(LSTM(20,dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dropout(0.3))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "optimzer=Adam(learning_rate=1e-4)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy', 'mae'])"
      ],
      "metadata": {
        "id": "S0Udpx2nYACq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_tweet[:7613], train['target'].values, epochs = 11)"
      ],
      "metadata": {
        "id": "GSt7E8R9YBrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "XmjyvwaUYDA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(padded_tweet[7613:])"
      ],
      "metadata": {
        "id": "5mAeCSS3YEoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds[:20]"
      ],
      "metadata": {
        "id": "19SJDcPLYIah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.round(preds).astype(int).reshape(3263)"
      ],
      "metadata": {
        "id": "jo6tVEzfYJoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "jaaV2heYYMFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission=pd.DataFrame()\n",
        "submission['id']=test['id'].to_list()\n",
        "submission['target']=pred"
      ],
      "metadata": {
        "id": "6ObNxexrYN1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission.head(10)"
      ],
      "metadata": {
        "id": "KfAcvus-YPDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission.to_csv('submission4.csv',index=False)"
      ],
      "metadata": {
        "id": "PpOCx9cIYQjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}